
# ğŸ§  Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning

This repository contains the code for the paper:  
**"Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning"**  
by *Roger Creus Castanyer, Johan Obando-Ceron, Lu Li, Pierre-Luc Bacon, Glen Berseth, Aaron Courville, and Pablo Samuel Castro*  
*Work done at Mila Quebec AI Institute and University of Montreal.*

Published at NeurIPS 2025 (Spotlight â­ - top ~3% of submissions)

## ğŸ“„ **[Read the Full Paper on arXiv â–¶ï¸](https://arxiv.org/pdf/2506.15544.pdf)**
---

## ğŸ“¦ Installation

To set up the environment for running the main scripts:

```bash
conda create -n deeprl python=3.10 -y
conda activate deeprl
pip install -r requirements.txt
```

To run Atari experiments (`dqn.py`, `rainbow.py`), install additional dependencies:

```bash
pip install -r requirements_atari.txt
pip install -U typing_extensions
```

---

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ pqn.py          # PQN algorithm
â”œâ”€â”€ ppo.py          # PPO algorithm
â”œâ”€â”€ dqn.py          # DQN algorithm
â”œâ”€â”€ rainbow.py      # Rainbow algorithm
â”œâ”€â”€ cifar.py        # CIFAR-10 experiments (supervised learning & non-stationary SL)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ agent.py    # Base classes for PQN and PPO agents
â”‚   â”œâ”€â”€ encoder.py  # CNN encoders (AtariCNN, ImpalaCNN)
â”‚   â””â”€â”€ mlp.py      # MLP variants (FC, Residual, Multi-skip, DenseNet)
â””â”€â”€ utils/
    â”œâ”€â”€ compute_hns.py          # Compute Human Normalized Scores
    â”œâ”€â”€ args.py                 # CLI arguments for PPO and PQN
    â”œâ”€â”€ representation_dynamics.py # Metrics and logging for representation dynamics
    â”œâ”€â”€ utils.py                # General utilities (optimizers, scaling, etc.)
    â””â”€â”€ wrappers.py            # Atari environment wrappers
```

---

## ğŸš€ Example Usage

> All CLI arguments for PPO and PQN are found in `src/utils/args.py`  
> DQN, Rainbow, and CIFAR-10 scripts also expose similar CLI argument interfaces.
> Running any script without command line arguments will use the default baseline configuration settings.

### â–¶ï¸ Run PQN (baseline)
```bash
python src/pqn.py
```

### ğŸ”¬ Run PQN with Gradient Interventions
```bash
python src/pqn.py --mlp_type=multiskip_residual --use_ln --optimizer=kron
```

### ğŸ§— Run PPO with Gradient Interventions
```bash
python src/ppo.py --mlp_type=multiskip_residual --use_ln --optimizer=kron
```

### ğŸ® Run DQN with Gradient Interventions
```bash
python src/dqn.py --mlp_type=multiskip_residual --use_ln --optimizer=kron
```

### ğŸŒˆ Run Rainbow with Gradient Interventions
```bash
python src/rainbow.py --mlp_type=multiskip_residual --use_ln --optimizer=kron
```

### ğŸ–¼ï¸ Run CIFAR-10 Baseline (SL & Non-Stationary SL)
```bash
python src/cifar.py --mlp_type=multiskip_residual --use_ln --optimizer=kron
```

---

## ğŸ“„ Citation
Please cite the original [NeurIPS paper](https://arxiv.org/abs/2506.15544):

```
@article{castanyer2025stable,
  title={Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning},
  author={Castanyer, Roger Creus and Obando-Ceron, Johan and Li, Lu and Bacon, Pierre-Luc and Berseth, Glen and Courville, Aaron and Castro, Pablo Samuel},
  journal={arXiv preprint arXiv:2506.15544},
  year={2025}
}
```

